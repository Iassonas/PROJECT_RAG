{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48b9a53",
   "metadata": {},
   "source": [
    "# 🧠 \"Retrieval-Augmented Generation with LLaMA 3.1 and Wikipedia\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1475bee",
   "metadata": {},
   "source": [
    "### Packages & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86ef85ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iasso\\Desktop\\Project_RAG\\RAG\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# STEP 1\n",
    "from datasets import load_dataset\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# STEP 2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# STEP 3\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# STEP 4 \n",
    "import accelerate\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# STEP 5 \n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3b91bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3abbe7d",
   "metadata": {},
   "source": [
    "## STEP 1: Load & Split Wikipedia into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05ce9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (205328, 4)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikipedia\", \"20220301.simple\", trust_remote_code=True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29e90b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May. It is one of four months to have 30 days.\n",
      "\n",
      "April always begins on the same day of week as July, and additionally, January in leap years. April always ends on the same day of the week as December.\n",
      "\n",
      "April's flowers are the Sweet Pea and Daisy. Its birthstone is the diamond. The meaning of the diamond is innocence.\n",
      "\n",
      "The Month \n",
      "\n",
      "April comes between March and May, making it the fourth month o...\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['text'][:500] +'...')  # First article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc273d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 654208\n",
      "Example chunk:\n",
      " April is the fourth month of the year in the Julian and Gregorian calendars, and comes between March and May. It is one of four months to have 30 days.\n",
      "\n",
      "April always begins on the same day of week as July, and additionally, January in leap years. April always ends on the same day of the week as December.\n",
      "\n",
      "April's flowers are the Sweet Pea and Daisy. Its birthstone is the diamond. The meaning of the diamond is innocence.\n",
      "\n",
      "The Month\n"
     ]
    }
   ],
   "source": [
    "# Load a small sample for testing\n",
    "#texts = [d['text'] for d in dataset['train'].select(range(10000))]  # First 10000 articles\n",
    "texts = [d['text'] for d in dataset['train']]  # All articles\n",
    "\n",
    "# Initialize the splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split each article into chunks\n",
    "#all_chunks = []\n",
    "#for text in texts:\n",
    "#    chunks = text_splitter.split_text(text)\n",
    "#    all_chunks.extend(chunks)\n",
    "\n",
    "# Optimized chunks only size >50\n",
    "all_chunks = [chunk for text in texts for chunk in text_splitter.split_text(text) if len(chunk.strip()) > 50]\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(\"Example chunk:\\n\", all_chunks[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b28ce93",
   "metadata": {},
   "source": [
    "## STEP 2: Batch Embedding of Text Chunks Using all-MiniLM-L6-v2 for Vector Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212de098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 20444/20444 [08:08<00:00, 41.81it/s] \n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained embedding model\n",
    "model2 = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')  # Fast & good for semantic search\n",
    "\n",
    "# Limit to first N chunks for testing (you can expand later)\n",
    "#sample_chunks = all_chunks[:1000]\n",
    "sample_chunks = all_chunks[:]\n",
    "\n",
    "# Compute embeddings (batch mode)\n",
    "embeddings = model2.encode(sample_chunks, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a1ee13",
   "metadata": {},
   "source": [
    "✅ Optional: Save for Later Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "914f7563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings and chunks\n",
    "np.save(\"embeddings.npy\", embeddings)\n",
    "with open(\"chunks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(sample_chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec5e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings = np.load(\"embeddings.npy\")\n",
    "\n",
    "# Load text chunks\n",
    "with open(\"chunks.pkl\", \"rb\") as f:\n",
    "    sample_chunks = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d648978",
   "metadata": {},
   "source": [
    "## STEP 3: Building a Metadata-Enriched Vector Index for Semantic Retrieval with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac71e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iasso\\AppData\\Local\\Temp\\ipykernel_39084\\727130913.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "# Convert your chunks into Document objects\n",
    "#documents = [Document(page_content=chunk) for chunk in sample_chunks]\n",
    "\n",
    "# Convert your chunks into Document objects with meta data\n",
    "documents = [Document(page_content=chunk, metadata={\"source\": f\"wiki_{i}\"}) for i, chunk in enumerate(sample_chunks)]\n",
    "\n",
    "# Reuse your sentence-transformers model as a LangChain embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Create FAISS index\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Save index locally\n",
    "vectorstore.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ee9549",
   "metadata": {},
   "source": [
    "🔍 To Load the Index Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93a1a0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse your sentence-transformers model as a LangChain embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
    "\n",
    "# Load saved FAISS index\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"faiss_index\",\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c594807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Query: What is artificial intelligence?\n",
      "\n",
      "--- Result 1 (Score: 0.3028) ---\n",
      "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955.\n",
      "\n",
      "--- Result 2 (Score: 0.3395) ---\n",
      "In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.\n",
      "\n",
      "--- Result 3 (Score: 0.5215) ---\n",
      "Related pages\n",
      " Neural networks\n",
      " Expert systems\n",
      " Machine learning\n",
      "\n",
      "References\n",
      "What is Artificial Intelligence (A.I)?\n",
      "Artificial intelligence\n",
      "https://aiscite.blogspot.com/2021/08/what-is-artificial-intelligence-in.html\n",
      "\n",
      "🔎 Query: What is a neural network?\n",
      "\n",
      "--- Result 1 (Score: 0.3748) ---\n",
      "A neural network (also called an ANN or an artificial neural network) is a sort of computer software, inspired by biological neurons. Biological brains are capable of solving difficult problems, but each neuron is only responsible for solving a very small part of the problem. Similarly, a neural network is made up of cells that work together to produce a desired result, although each individual cell is only responsible for solving a small part of the problem\n",
      "\n",
      "--- Result 2 (Score: 0.4470) ---\n",
      "Overview \n",
      "\n",
      "A neural network models a network of neurons, like those in the human brain.\n",
      "Each neuron does simple mathematical operations: it receives data from other neurons, modifies it and sends it to other neurons.\n",
      "Neurons are placed in \"layers\": a neuron from a layer receives data from the neurons of other layers, modifies it and sends data to the neurons of other layers.\n",
      "A neural network is made up of one or more layers.\n",
      "\n",
      "--- Result 3 (Score: 0.4557) ---\n",
      "Neural network can be:\n",
      "\n",
      "Artificial neural network, a computer simulation of the way a biological brain works.\n",
      "Biological neural network, a neuroscience term for a group of neurons connected to one another.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is artificial intelligence?\"\n",
    "query2 = \"What is a neural network?\"\n",
    "\n",
    "# Get results with similarity scores\n",
    "docs_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
    "docs2_scores = vectorstore.similarity_search_with_score(query2, k=3)\n",
    "\n",
    "# Print top 3 results for the first query\n",
    "print(f\"\\n🔎 Query: {query}\")\n",
    "for i, (doc, score) in enumerate(docs_scores):\n",
    "    print(f\"\\n--- Result {i+1} (Score: {score:.4f}) ---\\n{doc.page_content[:500]}\")\n",
    "\n",
    "# Print top 3 results for the second query\n",
    "print(f\"\\n🔎 Query: {query2}\")\n",
    "for i, (doc, score) in enumerate(docs2_scores):\n",
    "    print(f\"\\n--- Result {i+1} (Score: {score:.4f}) ---\\n{doc.page_content[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c3838",
   "metadata": {},
   "source": [
    "## STEP 4: Quantized Inference with LLaMA 3.1 8B and Contextual Prompting via FAISS Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d2bba6",
   "metadata": {},
   "source": [
    "### 8-bit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# 🔧 Quantization config (8-bit via bitsandbytes)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# ✅ Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 🔍 Prompt\n",
    "prompt = \"Explain how solar panels generate electricity in simple terms.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(\"\\n🧠 LLaMA 3.1 Response:\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d937ce",
   "metadata": {},
   "source": [
    "### Delete the model if it exists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6284167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Delete the 8-bit model if it exists (offload GPU)\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e0c43",
   "metadata": {},
   "source": [
    "### 4-bit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc24314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:20<00:00,  5.12s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 LLaMA 3.1 Response:\n",
      "\n",
      "Explain how solar panels generate electricity in simple terms. Solar panels convert sunlight into electricity by using special cells called photovoltaic cells. These cells contain tiny particles called electrons that are excited by the sunlight and flow through a circuit, creating an electric current. This process is known as the photovoltaic effect.\n",
      "Solar panels are made up of many photovoltaic cells that are connected together to form a panel. When sunlight hits the cells, it excites the electrons, which then flow through a circuit and generate electricity. The electricity is then sent through an inverter, which converts the DC power into AC power, making it usable for homes and businesses.\n",
      "Solar panels are a clean and renewable source of energy, producing no emissions or pollution. They are also a cost-effective way to generate electricity, as they can save homeowners and businesses money on their energy bills.\n",
      "In simple terms, solar panels work by:\n",
      "1. Converting sunlight into electricity using photovoltaic cells.\n",
      "2. Exciting electrons with sunlight, which flow through a circuit.\n",
      "3. Generating\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# 🔧 Quantization config (4-bit via bitsandbytes)\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # or torch.float16\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 🔍 Prompt\n",
    "prompt = \"Explain how solar panels generate electricity in simple terms.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(\"\\n🧠 LLaMA 3.1 Response:\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18868530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, retriever):\n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs[:3])\n",
    "    \n",
    "    # Step 2: Format prompt\n",
    "    prompt = f\"Answer the question based on the context.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    # Step 3: Tokenize and generate\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166340c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iasso\\AppData\\Local\\Temp\\ipykernel_39084\\2999832903.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context.\n",
      "\n",
      "Context:\n",
      "Artificial intelligence (AI) is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955.\n",
      "\n",
      "In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.\n",
      "\n",
      "Related pages\n",
      " Neural networks\n",
      " Expert systems\n",
      " Machine learning\n",
      "\n",
      "References\n",
      "What is Artificial Intelligence (A.I)?\n",
      "Artificial intelligence\n",
      "https://aiscite.blogspot.com/2021/08/what-is-artificial-intelligence-in.html\n",
      "\n",
      "Question: What is artificial intelligence?\n",
      "Answer: Artificial intelligence is the ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be done by computers, though not in the same way as we do. Andreas Kaplan and Michael Haenlein define AI as a system’s ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.  Answer: The ability of a computer program or a machine to think and learn. It is also a field of study which tries to make computers \"smart\". They work on their own without being encoded with commands. John McCarthy came up with the name \"Artificial Intelligence\" in 1955. In general use, the term \"artificial intelligence\" means a programme which mimics human cognition. At least some of the things we associate with other minds, such as learning and problem solving can be\n"
     ]
    }
   ],
   "source": [
    "response = generate_answer(\"What is artificial intelligence?\", vectorstore.as_retriever())\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cbc508",
   "metadata": {},
   "source": [
    "## STEP 5: Real-Time Semantic Chat with Contextual Answering via Vector Retrieval and LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5191ad11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iasso\\Desktop\\Project_RAG\\RAG\\Lib\\site-packages\\gradio\\chat_interface.py:339: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://a50130bf9dc3e5fff8.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a50130bf9dc3e5fff8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def generate_answer(message, history):\n",
    "    query = message  # Get user's current message\n",
    "\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return \"Sorry, I couldn't find relevant information to answer your question.\"\n",
    "    \n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs[:3])\n",
    "\n",
    "    prompt = f\"Answer the question based on the context.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=4096).to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=generate_answer,\n",
    "    title=\"🧠 LLaMA 3.1 RAG Chatbot\",\n",
    "    description=\"Ask anything based on Wikipedia (Simple English).\",\n",
    "    theme=\"soft\",\n",
    ").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
